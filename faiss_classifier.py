#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import json
import os
from typing import List, Dict, Any, Optional

# FAISS ÏûÑÌè¨Ìä∏ (Streamlit Cloud Ìò∏Ìôò)
try:
    import faiss
    FAISS_AVAILABLE = True
    print("‚úÖ FAISS ÏÇ¨Ïö© Í∞ÄÎä•")
except ImportError as e:
    print(f"‚ùå FAISS ÏÑ§Ïπò ÌïÑÏöî: pip install faiss-cpu")
    FAISS_AVAILABLE = False

# Streamlit Cloud ÌôòÍ≤Ω ÌôïÏù∏
import os
if os.getenv('STREAMLIT_CLOUD'):
    print("üåê Streamlit Cloud ÌôòÍ≤Ω Í∞êÏßÄ")
    if not FAISS_AVAILABLE:
        print("‚ö†Ô∏è Streamlit CloudÏóêÏÑú FAISS ÏÇ¨Ïö© Î∂àÍ∞Ä, ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÎ•òÎ°ú Ìè¥Î∞±")

# sentence-transformers ÏûÑÌè¨Ìä∏
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
    print("‚úÖ sentence-transformers ÏÇ¨Ïö© Í∞ÄÎä•")
except ImportError as e:
    print(f"‚ùå sentence-transformers ÏÑ§Ïπò ÌïÑÏöî: pip install sentence-transformers")
    SENTENCE_TRANSFORMERS_AVAILABLE = False

class FaissVectorClassifier:
    def __init__(self, persist_directory: str = "faiss_issue_classification"):
        """FAISS Í∏∞Î∞ò Î≤°ÌÑ∞ Î∂ÑÎ•òÍ∏∞ Ï¥àÍ∏∞Ìôî"""
        self.persist_directory = persist_directory
        self.issue_types = [
            "ÌòÑÏû¨ ÎπÑÎ∞ÄÎ≤àÌò∏Í∞Ä ÎßûÏßÄ ÏïäÏäµÎãàÎã§",
            "VMSÏôÄÏùò ÌÜµÏã†Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§", 
            "Ping ÌÖåÏä§Ìä∏Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§",
            "Onvif ÏùëÎãµÏù¥ ÏóÜÏäµÎãàÎã§",
            "Î°úÍ∑∏Ïù∏ Ï∞®Îã® ÏÉÅÌÉúÏûÖÎãàÎã§",
            "ÎπÑÎ∞ÄÎ≤àÌò∏ Î≥ÄÍ≤ΩÏóê Ïã§Ìå®ÌñàÏäµÎãàÎã§",
            "PK P Í≥ÑÏ†ï Î°úÍ∑∏Ïù∏ ÏïàÎê®",
            "PK P Ïõπ Ï†ëÏÜç ÏïàÎê®",
            "Í∏∞ÌÉÄ"
        ]
        
        # ÌÇ§ÏõåÎìú Îß§Ìïë (Ìè¥Î∞±Ïö©)
        self.keyword_mapping = {
            "ÌòÑÏû¨ ÎπÑÎ∞ÄÎ≤àÌò∏Í∞Ä ÎßûÏßÄ ÏïäÏäµÎãàÎã§": [
                "ÎπÑÎ∞ÄÎ≤àÌò∏", "Ìå®Ïä§ÏõåÎìú", "password", "Ïù∏Ï¶ù", "Î°úÍ∑∏Ïù∏", "Ï†ëÏÜç", "Ïõπ", "cctv",
                "ÎßûÏßÄ Ïïä", "ÌãÄÎ†∏", "Ïã§Ìå®", "Ïò§Î•ò", "Ïù∏Ï¶ù Ïã§Ìå®", "Ï†ëÏÜç Ïã§Ìå®", "Î°úÍ∑∏Ïù∏ Ïã§Ìå®"
            ],
            "VMSÏôÄÏùò ÌÜµÏã†Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§": [
                "vms", "VMS", "ÌÜµÏã†", "Ïó∞Í≤∞", "ÏÑúÎ≤Ñ", "svms", "nvr", "ÏòÅÏÉÅ", "Ïπ¥Î©îÎùº",
                "Ïã§Ìå®", "Ïò§Î•ò", "Ïó∞Í≤∞ Ïïà", "ÌÜµÏã† Ïã§Ìå®", "ÏÑúÎ≤Ñ Ïó∞Í≤∞"
            ],
            "Ping ÌÖåÏä§Ìä∏Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§": [
                "ping", "Ping", "ÎÑ§Ìä∏ÏõåÌÅ¨", "Ïó∞Í≤∞", "ÌÜµÏã†", "ÌÖåÏä§Ìä∏", "ÏùëÎãµ", "ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞",
                "Ïã§Ìå®", "ÏïàÎê®", "Ïò§Î•ò", "ping Ïã§Ìå®", "ÎÑ§Ìä∏ÏõåÌÅ¨ Ïã§Ìå®"
            ],
            "Onvif ÏùëÎãµÏù¥ ÏóÜÏäµÎãàÎã§": [
                "onvif", "Onvif", "ONVIF", "ÌîÑÎ°úÌÜ†ÏΩú", "Ïπ¥Î©îÎùº", "ÏÑ§Ï†ï", "ÌÜµÏã†", "ÏùëÎãµ",
                "ÏóÜ", "Ïã§Ìå®", "Ïò§Î•ò", "onvif ÏùëÎãµ", "ÌîÑÎ°úÌÜ†ÏΩú ÏùëÎãµ"
            ],
            "Î°úÍ∑∏Ïù∏ Ï∞®Îã® ÏÉÅÌÉúÏûÖÎãàÎã§": [
                "Ï∞®Îã®", "Ïû†Í∏à", "Î°úÍ∑∏Ïù∏", "Í≥ÑÏ†ï", "cctv", "Ï†ëÏÜç", "Ï∞®Îã® ÏÉÅÌÉú", "Ïû†Í∏à ÏÉÅÌÉú",
                "ÏïàÎê®", "Ïã§Ìå®", "Ïò§Î•ò", "Î°úÍ∑∏Ïù∏ Ï∞®Îã®", "Í≥ÑÏ†ï Ï∞®Îã®"
            ],
            "ÎπÑÎ∞ÄÎ≤àÌò∏ Î≥ÄÍ≤ΩÏóê Ïã§Ìå®ÌñàÏäµÎãàÎã§": [
                "ÎπÑÎ∞ÄÎ≤àÌò∏ Î≥ÄÍ≤Ω", "Ìå®Ïä§ÏõåÎìú Î≥ÄÍ≤Ω", "Î≥ÄÍ≤Ω", "ÏàòÏ†ï", "ÏóÖÎç∞Ïù¥Ìä∏", "ÎπÑÎ∞ÄÎ≤àÌò∏ ÏàòÏ†ï",
                "Ïã§Ìå®", "ÏïàÎê®", "Ïò§Î•ò", "Î≥ÄÍ≤Ω Ïã§Ìå®", "ÏàòÏ†ï Ïã§Ìå®"
            ],
            "PK P Í≥ÑÏ†ï Î°úÍ∑∏Ïù∏ ÏïàÎê®": [
                "pk p", "PK P", "Í≥ÑÏ†ï", "Î°úÍ∑∏Ïù∏", "30Ïùº", "ÎØ∏Ï†ëÏÜç", "Ïû†Í∏à", "Ïû†Í≤º",
                "ÏïàÎê®", "Ïã§Ìå®", "Ïò§Î•ò", "Í≥ÑÏ†ï Î°úÍ∑∏Ïù∏", "pk p Í≥ÑÏ†ï"
            ],
            "PK P Ïõπ Ï†ëÏÜç ÏïàÎê®": [
                "pk p", "PK P", "Ïõπ", "Ï†ëÏÜç", "ÏõπÏÇ¨Ïù¥Ìä∏", "ÌÜ∞Ï∫£", "tomcat", "ÏÑúÎπÑÏä§",
                "ÏïàÎê®", "Ïã§Ìå®", "Ïò§Î•ò", "Ïõπ Ï†ëÏÜç", "ÌéòÏù¥ÏßÄ", "Î°úÎìú"
            ]
        }
        
        # FAISS Ïù∏Îç±Ïä§ÏôÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
        self.index = None
        self.documents = []
        self.metadatas = []
        self.embedding_model = None
        
        # Ï¥àÍ∏∞Ìôî
        self._initialize_embedding_model()
        self._load_or_create_index()
    
    def _initialize_embedding_model(self):
        """ÏûÑÎ≤†Îî© Î™®Îç∏ Ï¥àÍ∏∞Ìôî"""
        try:
            if SENTENCE_TRANSFORMERS_AVAILABLE:
                # Í≤ΩÎüâ Î™®Îç∏ ÏÇ¨Ïö© (WindowsÏóêÏÑú Îçî ÏïàÏ†ïÏ†Å)
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                print("‚úÖ sentence-transformers Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ")
            else:
                print("‚ö†Ô∏è sentence-transformers ÏóÜÏùå, ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÎ•òÎßå ÏÇ¨Ïö©")
                self.embedding_model = None
        except Exception as e:
            print(f"‚ùå ÏûÑÎ≤†Îî© Î™®Îç∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            self.embedding_model = None
    
    def _load_or_create_index(self):
        """FAISS Ïù∏Îç±Ïä§ Î°úÎìú ÎòêÎäî ÏÉùÏÑ±"""
        try:
            if not FAISS_AVAILABLE or not self.embedding_model:
                print("‚ö†Ô∏è FAISS ÎòêÎäî ÏûÑÎ≤†Îî© Î™®Îç∏ ÏóÜÏùå, ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÎ•òÎßå ÏÇ¨Ïö©")
                return
            
            # Ï†ÄÏû•Îêú Ïù∏Îç±Ïä§ Î°úÎìú ÏãúÎèÑ
            index_path = os.path.join(self.persist_directory, "faiss_index.bin")
            metadata_path = os.path.join(self.persist_directory, "metadata.json")
            
            if os.path.exists(index_path) and os.path.exists(metadata_path):
                self.index = faiss.read_index(index_path)
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.documents = data['documents']
                    self.metadatas = data['metadatas']
            else:
                self._create_index()
                
        except Exception as e:
            print(f"‚ùå FAISS Ïù∏Îç±Ïä§ Î°úÎìú/ÏÉùÏÑ± Ïã§Ìå®: {e}")
            self.index = None
    
    def _create_index(self):
        """ÏÉàÎ°úÏö¥ FAISS Ïù∏Îç±Ïä§ ÏÉùÏÑ±"""
        try:
            # ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ Î°úÎìú
            sample_data = self._load_sample_data()
            
            if not sample_data:
                print("‚ùå ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå")
                return
            
            # Î¨∏ÏÑúÏôÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
            documents = []
            metadatas = []
            
            for issue_type, samples in sample_data.items():
                for i, sample in enumerate(samples):
                    documents.append(sample)
                    metadatas.append({
                        "issue_type": issue_type,
                        "sample_index": i,
                        "is_sample": True
                    })
            
            # ÏûÑÎ≤†Îî© ÏÉùÏÑ±
            embeddings = self.embedding_model.encode(documents)
            embeddings = embeddings.astype('float32')
            
            # FAISS Ïù∏Îç±Ïä§ ÏÉùÏÑ± (ÎÇ¥Ï†Å Í∏∞Î∞ò Ïú†ÏÇ¨ÎèÑ)
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            self.index.add(embeddings)
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
            self.documents = documents
            self.metadatas = metadatas
            
            # ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
            os.makedirs(self.persist_directory, exist_ok=True)
            
            # Ïù∏Îç±Ïä§ Ï†ÄÏû•
            faiss.write_index(self.index, os.path.join(self.persist_directory, "faiss_index.bin"))
            with open(os.path.join(self.persist_directory, "metadata.json"), 'w', encoding='utf-8') as f:
                json.dump({
                    'documents': self.documents,
                    'metadatas': self.metadatas
                }, f, ensure_ascii=False, indent=2)
            
            
        except Exception as e:
            print(f"‚ùå FAISS Ïù∏Îç±Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            self.index = None
    
    def _load_sample_data(self):
        """ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ Î°úÎìú (Streamlit Cloud Ìò∏Ìôò)"""
        try:
            # Streamlit resourceÎ•º ÌÜµÌïú ÌååÏùº ÏùΩÍ∏∞ ÏãúÎèÑ
            try:
                import streamlit as st
                # Streamlit CloudÏóêÏÑú ÌååÏùº ÏùΩÍ∏∞
                with open("vector_data/sample_issues.json", 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get("sample_issues", {})
            except:
                pass
            
            # Ïó¨Îü¨ Í≤ΩÎ°ú ÏãúÎèÑ (Î°úÏª¨ ÌôòÍ≤Ω Ìò∏Ìôò)
            possible_paths = [
                os.path.join(os.path.dirname(__file__), "vector_data", "sample_issues.json"),
                os.path.join("vector_data", "sample_issues.json"),
                "vector_data/sample_issues.json",
                os.path.join(os.getcwd(), "vector_data", "sample_issues.json")
            ]
            
            for json_path in possible_paths:
                if os.path.exists(json_path):
                    with open(json_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        return data.get("sample_issues", {})
            
            print("‚ö†Ô∏è ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå, Í∏∞Î≥∏ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©")
            # Í∏∞Î≥∏ Îç∞Ïù¥ÌÑ∞ (sample_issues.jsonÍ≥º ÎèôÏùº)
            return {
                "ÌòÑÏû¨ ÎπÑÎ∞ÄÎ≤àÌò∏Í∞Ä ÎßûÏßÄ ÏïäÏäµÎãàÎã§": [
                    "CCTV Ïõπ Ï†ëÏÜç Ïãú ÎπÑÎ∞ÄÎ≤àÌò∏ Ïù∏Ï¶ù Ïã§Ìå®",
                    "Ï†ÄÏû•Îêú ÎπÑÎ∞ÄÎ≤àÌò∏Î°ú Î°úÍ∑∏Ïù∏Ïù¥ ÏïàÎê©ÎãàÎã§",
                    "ÎπÑÎ∞ÄÎ≤àÌò∏Î•º Ï†ïÌôïÌûà ÏûÖÎ†•ÌñàÎäîÎç∞ÎèÑ Ïù∏Ï¶ù Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§",
                    "CCTV Ïõπ Î°úÍ∑∏Ïù∏ Ïãú Ï†ëÏÜç Ïã§Ìå®",
                    "Ìå®Ïä§ÏõåÎìúÍ∞Ä ÎßûÏßÄ ÏïäÏïÑ Î°úÍ∑∏Ïù∏Ìï† Ïàò ÏóÜÏäµÎãàÎã§",
                    "Ïõπ Ï†ëÏÜç Ïãú Ïù∏Ï¶ù Ïã§Ìå®Í∞Ä Í≥ÑÏÜç Î∞úÏÉùÌï©ÎãàÎã§"
                ],
                "VMSÏôÄÏùò ÌÜµÏã†Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§": [
                    "VMS ÏÑúÎ≤ÑÏôÄÏùò Ïó∞Í≤∞Ïù¥ ÏïàÎê©ÎãàÎã§",
                    "VMS Ìå®Ïä§ÏõåÎìúÍ∞Ä ÎßûÏßÄ ÏïäÏäµÎãàÎã§",
                    "SVMS ÌÜµÏã† Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§",
                    "NVRÍ≥º VMS Í∞Ñ ÌÜµÏã† Ïã§Ìå®",
                    "VMS ÏÑ§Ï†ïÏóêÏÑú Ïó∞Í≤∞ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§",
                    "VMS ÏÑúÎ≤Ñ Ïó∞Í≤∞Ïù¥ ÎÅäÏñ¥Ï°åÏäµÎãàÎã§"
                ],
                "Ping ÌÖåÏä§Ìä∏Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§": [
                    "ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞Ïù¥ ÏïàÎê©ÎãàÎã§",
                    "Ping ÌÖåÏä§Ìä∏ÏóêÏÑú ÏùëÎãµÏù¥ ÏóÜÏäµÎãàÎã§",
                    "ÎÑ§Ìä∏ÏõåÌÅ¨ ÌÜµÏã†Ïù¥ Î∂àÏïàÏ†ïÌï©ÎãàÎã§",
                    "Ïó∞Í≤∞ ÏÉÅÌÉúÎ•º ÌôïÏù∏Ìï† Ïàò ÏóÜÏäµÎãàÎã§",
                    "ÎÑ§Ìä∏ÏõåÌÅ¨ Ï†êÍ≤ÄÏóêÏÑú Ïã§Ìå®Í∞Ä Î∞úÏÉùÌï©ÎãàÎã§",
                    "ÌÜµÏã† ÌÖåÏä§Ìä∏Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§"
                ],
                "Onvif ÏùëÎãµÏù¥ ÏóÜÏäµÎãàÎã§": [
                    "Onvif ÌîÑÎ°úÌÜ†ÏΩú ÏùëÎãµÏù¥ ÏóÜÏäµÎãàÎã§",
                    "Ïπ¥Î©îÎùºÏôÄ Onvif ÌÜµÏã†Ïù¥ ÏïàÎê©ÎãàÎã§",
                    "Onvif ÏÑ§Ï†ïÏóêÏÑú Ïó∞Í≤∞ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§",
                    "Ïπ¥Î©îÎùº Onvif ÏÑúÎπÑÏä§Í∞Ä ÏùëÎãµÌïòÏßÄ ÏïäÏäµÎãàÎã§",
                    "Onvif ÌîÑÎ°úÌÜ†ÏΩúÎ°ú Ïπ¥Î©îÎùº Ï†ëÏÜçÏù¥ ÏïàÎê©ÎãàÎã§",
                    "Onvif ÌÜµÏã† ÌÖåÏä§Ìä∏Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§"
                ],
                "Î°úÍ∑∏Ïù∏ Ï∞®Îã® ÏÉÅÌÉúÏûÖÎãàÎã§": [
                    "CCTV Î°úÍ∑∏Ïù∏Ïù¥ Ï∞®Îã®ÎêòÏóàÏäµÎãàÎã§",
                    "ÎπÑÎ∞ÄÎ≤àÌò∏ Î≥ÄÍ≤Ω ÌõÑ Î°úÍ∑∏Ïù∏ Ï∞®Îã® ÏÉÅÌÉúÏûÖÎãàÎã§",
                    "Í≥ÑÏ†ïÏù¥ Ï∞®Îã®ÎêòÏñ¥ Î°úÍ∑∏Ïù∏Ìï† Ïàò ÏóÜÏäµÎãàÎã§",
                    "CCTV Ï∞®Îã® ÏÉÅÌÉúÎ°ú Ï†ëÏÜçÏù¥ ÏïàÎê©ÎãàÎã§",
                    "Î°úÍ∑∏Ïù∏ ÏãúÎèÑÍ∞Ä Ï∞®Îã®ÎêòÏóàÏäµÎãàÎã§",
                    "ÎπÑÎ∞ÄÎ≤àÌò∏ Î≥ÄÍ≤ΩÏúºÎ°ú Ïù∏Ìïú Ï∞®Îã® ÏÉÅÌÉúÏûÖÎãàÎã§"
                ],
                "ÎπÑÎ∞ÄÎ≤àÌò∏ Î≥ÄÍ≤ΩÏóê Ïã§Ìå®ÌñàÏäµÎãàÎã§": [
                    "ÎπÑÎ∞ÄÎ≤àÌò∏ Î≥ÄÍ≤ΩÏù¥ ÏïàÎê©ÎãàÎã§",
                    "Ìå®Ïä§ÏõåÎìú ÏàòÏ†ïÏóê Ïã§Ìå®ÌñàÏäµÎãàÎã§",
                    "ÎπÑÎ∞ÄÎ≤àÌò∏ ÏóÖÎç∞Ïù¥Ìä∏ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§",
                    "ÎπÑÎ∞ÄÎ≤àÌò∏ Î≥ÄÍ≤Ω Ïãú Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§",
                    "Ìå®Ïä§ÏõåÎìú Î≥ÄÍ≤Ω ÌîÑÎ°úÏÑ∏Ïä§Í∞Ä Ïã§Ìå®ÌñàÏäµÎãàÎã§",
                    "ÎπÑÎ∞ÄÎ≤àÌò∏ ÏàòÏ†ïÏù¥ Ï†úÎåÄÎ°ú ÎêòÏßÄ ÏïäÏäµÎãàÎã§"
                ],
                "PK P Í≥ÑÏ†ï Î°úÍ∑∏Ïù∏ ÏïàÎê®": [
                    "PK P Í≥ÑÏ†ïÏúºÎ°ú Î°úÍ∑∏Ïù∏Ïù¥ ÏïàÎê©ÎãàÎã§",
                    "30Ïùº ÎØ∏Ï†ëÏÜçÏúºÎ°ú Í≥ÑÏ†ïÏù¥ Ïû†Í≤ºÏäµÎãàÎã§",
                    "PK P Í≥ÑÏ†ïÏù¥ Ïû†Í≤®ÏûàÏäµÎãàÎã§",
                    "Í≥ÑÏ†ï Î°úÍ∑∏Ïù∏Ïóê Ïã§Ìå®Ìï©ÎãàÎã§",
                    "PK P Í≥ÑÏ†ï Ïù∏Ï¶ùÏù¥ ÏïàÎê©ÎãàÎã§",
                    "Í≥ÑÏ†ï Ï†ëÏÜçÏù¥ Ï∞®Îã®ÎêòÏóàÏäµÎãàÎã§"
                ],
                "PK P Ïõπ Ï†ëÏÜç ÏïàÎê®": [
                    "PK P ÏõπÏÇ¨Ïù¥Ìä∏Ïóê Ï†ëÏÜçÏù¥ ÏïàÎê©ÎãàÎã§",
                    "ÌÜ∞Ï∫£ ÏÑúÎπÑÏä§Í∞Ä Ï§ëÎã®ÎêòÏóàÏäµÎãàÎã§",
                    "PK P Ïõπ ÏÑúÎπÑÏä§Í∞Ä ÏùëÎãµÌïòÏßÄ ÏïäÏäµÎãàÎã§",
                    "Ïõπ Ï†ëÏÜç Ïãú Ïó∞Í≤∞ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§",
                    "PK P Ïõπ ÌéòÏù¥ÏßÄÍ∞Ä Î°úÎìúÎêòÏßÄ ÏïäÏäµÎãàÎã§",
                    "Ïõπ ÏÑúÎπÑÏä§ Ïó∞Í≤∞Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§"
                ],
                "Í∏∞ÌÉÄ": [
                    "Ïïå Ïàò ÏóÜÎäî Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§",
                    "ÏãúÏä§ÌÖú Ïò§Î•òÎ°ú Ïù∏Ìï¥ Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§",
                    "ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§",
                    "Í∏∞Ïà†Ï†Å Î¨∏Ï†úÎ°ú Ïù∏Ìï¥ ÏÑúÎπÑÏä§Í∞Ä Ï§ëÎã®ÎêòÏóàÏäµÎãàÎã§",
                    "ÏãúÏä§ÌÖú Ï†êÍ≤Ä Ï§ëÏûÖÎãàÎã§",
                    "ÏùºÏãúÏ†ÅÏù∏ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§"
                    ]
                }
        except Exception as e:
            print(f"‚ùå ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ïã§Ìå®: {e}")
            return {}
    
    def _classify_by_keywords(self, customer_input: str) -> Dict[str, Any]:
        """ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÎ•ò (Ìè¥Î∞±)"""
        try:
            normalized_input = customer_input.lower().strip()
            issue_scores = {}
            
            for issue_type, keywords in self.keyword_mapping.items():
                score = 0
                matched_keywords = []
                
                for keyword in keywords:
                    if keyword.lower() in normalized_input:
                        score += 1
                        matched_keywords.append(keyword)
                
                if score > 0:
                    issue_scores[issue_type] = {
                        'score': score,
                        'matched_keywords': matched_keywords,
                        'confidence': min(score / len(keywords), 1.0)
                    }
            
            if issue_scores:
                best_issue = max(issue_scores.items(), key=lambda x: x[1]['score'])
                best_issue_type = best_issue[0]
                best_score = best_issue[1]['score']
                best_confidence = best_issue[1]['confidence']
                matched_keywords = best_issue[1]['matched_keywords']
                
                confidence_level = 'high' if best_confidence >= 0.5 else 'medium' if best_confidence >= 0.3 else 'low'
                
                return {
                    'issue_type': best_issue_type,
                    'method': 'keyword_based',
                    'confidence': confidence_level,
                    'score': best_score,
                    'matched_keywords': matched_keywords,
                    'all_scores': {k: v['score'] for k, v in issue_scores.items()}
                }
            else:
                return {
                    'issue_type': 'Í∏∞ÌÉÄ',
                    'method': 'keyword_based',
                    'confidence': 'low',
                    'score': 0,
                    'matched_keywords': [],
                    'all_scores': {}
                }
                
        except Exception as e:
            return {
                'issue_type': 'Í∏∞ÌÉÄ',
                'method': 'keyword_based',
                'confidence': 'low',
                'error': str(e)
            }
    
    def classify_issue(self, customer_input: str, top_k: int = 3) -> Dict[str, Any]:
        """Î≤°ÌÑ∞ Í∏∞Î∞ò Î¨∏Ï†ú Ïú†Ìòï Î∂ÑÎ•ò (FAISS + ÌÇ§ÏõåÎìú Ìè¥Î∞±)"""
        try:
            
            # FAISS Î≤°ÌÑ∞ Î∂ÑÎ•ò ÏãúÎèÑ
            if self.index is not None and self.embedding_model is not None:
                
                # ÏøºÎ¶¨ ÏûÑÎ≤†Îî© ÏÉùÏÑ±
                query_embedding = self.embedding_model.encode([customer_input]).astype('float32')
                
                # FAISS Í≤ÄÏÉâ
                scores, indices = self.index.search(query_embedding, top_k)
                
                if len(indices[0]) > 0:
                    # Í≤∞Í≥º Î∂ÑÏÑù
                    issue_scores = {}
                    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                        if idx < len(self.metadatas):
                            issue_type = self.metadatas[idx]['issue_type']
                            if issue_type not in issue_scores:
                                issue_scores[issue_type] = []
                            issue_scores[issue_type].append(float(score))
                    
                    # ÏµúÍ≥† Ï†êÏàò Î¨∏Ï†ú Ïú†Ìòï ÏÑ†ÌÉù
                    best_issue_type = 'Í∏∞ÌÉÄ'
                    best_score = 0
                    
                    for issue_type, scores_list in issue_scores.items():
                        max_score = max(scores_list)
                        if max_score > best_score:
                            best_score = max_score
                            best_issue_type = issue_type
                    
                    # Ïã†Î¢∞ÎèÑ Í≤∞Ï†ï
                    if best_score >= 0.7:
                        confidence = 'high'
                    elif best_score >= 0.5:
                        confidence = 'medium'
                    else:
                        confidence = 'low'
                    
                    
                    return {
                        'issue_type': best_issue_type,
                        'method': 'faiss_vector',
                        'confidence': confidence,
                        'similarity_score': best_score,
                        'all_scores': {k: max(v) for k, v in issue_scores.items()}
                    }
            
            # FAISS Ïã§Ìå® Ïãú ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÎ•òÎ°ú Ìè¥Î∞±
            return self._classify_by_keywords(customer_input)
            
        except Exception as e:
            print(f"‚ùå Î∂ÑÎ•ò Ïã§Ìå®: {e}")
            return self._classify_by_keywords(customer_input)
    
    def get_statistics(self) -> Dict[str, Any]:
        """Î∂ÑÎ•òÍ∏∞ ÌÜµÍ≥Ñ"""
        try:
            if self.index is not None:
                # Î¨∏Ï†ú Ïú†ÌòïÎ≥Ñ ÌÜµÍ≥Ñ
                issue_type_counts = {}
                for metadata in self.metadatas:
                    issue_type = metadata.get('issue_type', 'Í∏∞ÌÉÄ')
                    issue_type_counts[issue_type] = issue_type_counts.get(issue_type, 0) + 1
                
                return {
                    'total_documents': len(self.documents),
                    'issue_types': list(issue_type_counts.keys()),
                    'issue_type_counts': issue_type_counts,
                    'index_size': self.index.ntotal if self.index else 0,
                    'method': 'faiss_vector',
                    'embedding_model': 'all-MiniLM-L6-v2' if self.embedding_model else 'None',
                    'collection_name': 'FAISS Vector DB'
                }
            else:
                # ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÎ•òÍ∏∞ ÌÜµÍ≥Ñ
                sample_data = self._load_sample_data()
                total_samples = sum(len(samples) for samples in sample_data.values())
                issue_type_counts = {issue_type: len(samples) for issue_type, samples in sample_data.items()}
                
                return {
                    'total_documents': total_samples,
                    'issue_types': list(issue_type_counts.keys()),
                    'issue_type_counts': issue_type_counts,
                    'index_size': 0,
                    'method': 'keyword_only',
                    'embedding_model': 'None',
                    'collection_name': 'N/A'
                }
        except Exception as e:
            print(f"‚ùå ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {
                'total_documents': 0,
                'issue_types': self.issue_types,
                'issue_type_counts': {},
                'index_size': 0,
                'method': 'keyword_only',
                'embedding_model': 'None',
                'collection_name': 'N/A'
            }

# ÌÖåÏä§Ìä∏ ÏΩîÎìú
if __name__ == "__main__":
    classifier = FaissVectorClassifier()
    
    test_cases = [
        "ÎπÑÎ∞ÄÎ≤àÌò∏Í∞Ä ÎßûÏßÄ ÏïäÏäµÎãàÎã§",
        "VMS ÌÜµÏã† Ïã§Ìå®",
        "Ping ÌÖåÏä§Ìä∏ Ïã§Ìå®",
        "Ïõπ Ï†ëÏÜç ÏïàÎê®"
    ]
    
    for test_input in test_cases:
        result = classifier.classify_issue(test_input)
        print(f"ÌÖåÏä§Ìä∏: {test_input} -> {result['issue_type']} ({result['method']}, {result['confidence']})")
    
    # ÌÜµÍ≥Ñ
    stats = classifier.get_statistics()
    print(f"Ï¥ù Î¨∏ÏÑú Ïàò: {stats['total_documents']}, Î∞©Î≤ï: {stats['method']}, Ïù∏Îç±Ïä§: {stats['index_size']}")
