#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import json
import os
from typing import List, Dict, Any, Optional

# FAISS ì„í¬íŠ¸ (Windowsì—ì„œ ë” ì•ˆì •ì )
try:
    import faiss
    FAISS_AVAILABLE = True
    print("âœ… FAISS ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    print(f"âŒ FAISS ì„¤ì¹˜ í•„ìš”: pip install faiss-cpu")
    FAISS_AVAILABLE = False

# sentence-transformers ì„í¬íŠ¸
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
    print("âœ… sentence-transformers ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    print(f"âŒ sentence-transformers ì„¤ì¹˜ í•„ìš”: pip install sentence-transformers")
    SENTENCE_TRANSFORMERS_AVAILABLE = False

class FaissVectorClassifier:
    def __init__(self, persist_directory: str = "faiss_issue_classification"):
        """FAISS ê¸°ë°˜ ë²¡í„° ë¶„ë¥˜ê¸° ì´ˆê¸°í™”"""
        self.persist_directory = persist_directory
        self.issue_types = [
            "í˜„ì¬ ë¹„ë°€ë²ˆí˜¸ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤",
            "VMSì™€ì˜ í†µì‹ ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤", 
            "Ping í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
            "Onvif ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤",
            "ë¡œê·¸ì¸ ì°¨ë‹¨ ìƒíƒœì…ë‹ˆë‹¤",
            "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
            "PK P ê³„ì • ë¡œê·¸ì¸ ì•ˆë¨",
            "PK P ì›¹ ì ‘ì† ì•ˆë¨",
            "ê¸°íƒ€"
        ]
        
        # í‚¤ì›Œë“œ ë§¤í•‘ (í´ë°±ìš©)
        self.keyword_mapping = {
            "í˜„ì¬ ë¹„ë°€ë²ˆí˜¸ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤": [
                "ë¹„ë°€ë²ˆí˜¸", "íŒ¨ìŠ¤ì›Œë“œ", "password", "ì¸ì¦", "ë¡œê·¸ì¸", "ì ‘ì†", "ì›¹", "cctv",
                "ë§ì§€ ì•Š", "í‹€ë ¸", "ì‹¤íŒ¨", "ì˜¤ë¥˜", "ì¸ì¦ ì‹¤íŒ¨", "ì ‘ì† ì‹¤íŒ¨", "ë¡œê·¸ì¸ ì‹¤íŒ¨"
            ],
            "VMSì™€ì˜ í†µì‹ ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤": [
                "vms", "VMS", "í†µì‹ ", "ì—°ê²°", "ì„œë²„", "svms", "nvr", "ì˜ìƒ", "ì¹´ë©”ë¼",
                "ì‹¤íŒ¨", "ì˜¤ë¥˜", "ì—°ê²° ì•ˆ", "í†µì‹  ì‹¤íŒ¨", "ì„œë²„ ì—°ê²°"
            ],
            "Ping í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤": [
                "ping", "Ping", "ë„¤íŠ¸ì›Œí¬", "ì—°ê²°", "í†µì‹ ", "í…ŒìŠ¤íŠ¸", "ì‘ë‹µ", "ë„¤íŠ¸ì›Œí¬ ì—°ê²°",
                "ì‹¤íŒ¨", "ì•ˆë¨", "ì˜¤ë¥˜", "ping ì‹¤íŒ¨", "ë„¤íŠ¸ì›Œí¬ ì‹¤íŒ¨"
            ],
            "Onvif ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤": [
                "onvif", "Onvif", "ONVIF", "í”„ë¡œí† ì½œ", "ì¹´ë©”ë¼", "ì„¤ì •", "í†µì‹ ", "ì‘ë‹µ",
                "ì—†", "ì‹¤íŒ¨", "ì˜¤ë¥˜", "onvif ì‘ë‹µ", "í”„ë¡œí† ì½œ ì‘ë‹µ"
            ],
            "ë¡œê·¸ì¸ ì°¨ë‹¨ ìƒíƒœì…ë‹ˆë‹¤": [
                "ì°¨ë‹¨", "ì ê¸ˆ", "ë¡œê·¸ì¸", "ê³„ì •", "cctv", "ì ‘ì†", "ì°¨ë‹¨ ìƒíƒœ", "ì ê¸ˆ ìƒíƒœ",
                "ì•ˆë¨", "ì‹¤íŒ¨", "ì˜¤ë¥˜", "ë¡œê·¸ì¸ ì°¨ë‹¨", "ê³„ì • ì°¨ë‹¨"
            ],
            "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤": [
                "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½", "íŒ¨ìŠ¤ì›Œë“œ ë³€ê²½", "ë³€ê²½", "ìˆ˜ì •", "ì—…ë°ì´íŠ¸", "ë¹„ë°€ë²ˆí˜¸ ìˆ˜ì •",
                "ì‹¤íŒ¨", "ì•ˆë¨", "ì˜¤ë¥˜", "ë³€ê²½ ì‹¤íŒ¨", "ìˆ˜ì • ì‹¤íŒ¨"
            ],
            "PK P ê³„ì • ë¡œê·¸ì¸ ì•ˆë¨": [
                "pk p", "PK P", "ê³„ì •", "ë¡œê·¸ì¸", "30ì¼", "ë¯¸ì ‘ì†", "ì ê¸ˆ", "ì ê²¼",
                "ì•ˆë¨", "ì‹¤íŒ¨", "ì˜¤ë¥˜", "ê³„ì • ë¡œê·¸ì¸", "pk p ê³„ì •"
            ],
            "PK P ì›¹ ì ‘ì† ì•ˆë¨": [
                "pk p", "PK P", "ì›¹", "ì ‘ì†", "ì›¹ì‚¬ì´íŠ¸", "í†°ìº£", "tomcat", "ì„œë¹„ìŠ¤",
                "ì•ˆë¨", "ì‹¤íŒ¨", "ì˜¤ë¥˜", "ì›¹ ì ‘ì†", "í˜ì´ì§€", "ë¡œë“œ"
            ]
        }
        
        # FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°
        self.index = None
        self.documents = []
        self.metadatas = []
        self.embedding_model = None
        
        # ì´ˆê¸°í™”
        self._initialize_embedding_model()
        self._load_or_create_index()
    
    def _initialize_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if SENTENCE_TRANSFORMERS_AVAILABLE:
                # ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš© (Windowsì—ì„œ ë” ì•ˆì •ì )
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                print("âœ… sentence-transformers ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            else:
                print("âš ï¸ sentence-transformers ì—†ìŒ, í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë§Œ ì‚¬ìš©")
                self.embedding_model = None
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.embedding_model = None
    
    def _load_or_create_index(self):
        """FAISS ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        try:
            if not FAISS_AVAILABLE or not self.embedding_model:
                print("âš ï¸ FAISS ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ ì—†ìŒ, í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë§Œ ì‚¬ìš©")
                return
            
            # ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            index_path = os.path.join(self.persist_directory, "faiss_index.bin")
            metadata_path = os.path.join(self.persist_directory, "metadata.json")
            
            if os.path.exists(index_path) and os.path.exists(metadata_path):
                print("ğŸ”„ ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
                self.index = faiss.read_index(index_path)
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.documents = data['documents']
                    self.metadatas = data['metadatas']
                print(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
            else:
                print("ğŸ”„ ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
                self._create_index()
                
        except Exception as e:
            print(f"âŒ FAISS ì¸ë±ìŠ¤ ë¡œë“œ/ìƒì„± ì‹¤íŒ¨: {e}")
            self.index = None
    
    def _create_index(self):
        """ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        try:
            # ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
            sample_data = self._load_sample_data()
            
            if not sample_data:
                print("âŒ ìƒ˜í”Œ ë°ì´í„° ì—†ìŒ")
                return
            
            # ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            documents = []
            metadatas = []
            
            for issue_type, samples in sample_data.items():
                for i, sample in enumerate(samples):
                    documents.append(sample)
                    metadatas.append({
                        "issue_type": issue_type,
                        "sample_index": i,
                        "is_sample": True
                    })
            
            # ì„ë² ë”© ìƒì„±
            print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = self.embedding_model.encode(documents)
            embeddings = embeddings.astype('float32')
            
            # FAISS ì¸ë±ìŠ¤ ìƒì„± (ë‚´ì  ê¸°ë°˜ ìœ ì‚¬ë„)
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            self.index.add(embeddings)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            self.documents = documents
            self.metadatas = metadatas
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.persist_directory, exist_ok=True)
            
            # ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(self.index, os.path.join(self.persist_directory, "faiss_index.bin"))
            with open(os.path.join(self.persist_directory, "metadata.json"), 'w', encoding='utf-8') as f:
                json.dump({
                    'documents': self.documents,
                    'metadatas': self.metadatas
                }, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
            
        except Exception as e:
            print(f"âŒ FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            self.index = None
    
    def _load_sample_data(self):
        """ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ"""
        try:
            json_path = os.path.join(os.path.dirname(__file__), "vector_data", "sample_issues.json")
            if os.path.exists(json_path):
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get("sample_issues", {})
            else:
                # ê¸°ë³¸ ë°ì´í„°
                return {
                    "í˜„ì¬ ë¹„ë°€ë²ˆí˜¸ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤": [
                        "CCTV ì›¹ ì ‘ì† ì‹œ ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ ì‹¤íŒ¨",
                        "ì €ì¥ëœ ë¹„ë°€ë²ˆí˜¸ë¡œ ë¡œê·¸ì¸ì´ ì•ˆë©ë‹ˆë‹¤",
                        "ë¹„ë°€ë²ˆí˜¸ë¥¼ ì •í™•íˆ ì…ë ¥í–ˆëŠ”ë°ë„ ì¸ì¦ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤"
                    ],
                    "VMSì™€ì˜ í†µì‹ ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤": [
                        "VMS ì„œë²„ì™€ì˜ ì—°ê²°ì´ ì•ˆë©ë‹ˆë‹¤",
                        "VMS íŒ¨ìŠ¤ì›Œë“œê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤",
                        "SVMS í†µì‹  ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤"
                    ],
                    "Ping í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤": [
                        "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ì•ˆë©ë‹ˆë‹¤",
                        "Ping í…ŒìŠ¤íŠ¸ì—ì„œ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤",
                        "ë„¤íŠ¸ì›Œí¬ í†µì‹ ì´ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤"
                    ],
                    "Onvif ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤": [
                        "Onvif í”„ë¡œí† ì½œ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤",
                        "ì¹´ë©”ë¼ì™€ Onvif í†µì‹ ì´ ì•ˆë©ë‹ˆë‹¤",
                        "Onvif ì„¤ì •ì—ì„œ ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤"
                    ],
                    "ë¡œê·¸ì¸ ì°¨ë‹¨ ìƒíƒœì…ë‹ˆë‹¤": [
                        "CCTV ë¡œê·¸ì¸ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤",
                        "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ í›„ ë¡œê·¸ì¸ ì°¨ë‹¨ ìƒíƒœì…ë‹ˆë‹¤",
                        "ê³„ì •ì´ ì°¨ë‹¨ë˜ì–´ ë¡œê·¸ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                    ],
                    "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤": [
                        "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ì´ ì•ˆë©ë‹ˆë‹¤",
                        "íŒ¨ìŠ¤ì›Œë“œ ìˆ˜ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
                        "ë¹„ë°€ë²ˆí˜¸ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤"
                    ],
                    "PK P ê³„ì • ë¡œê·¸ì¸ ì•ˆë¨": [
                        "PK P ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸ì´ ì•ˆë©ë‹ˆë‹¤",
                        "30ì¼ ë¯¸ì ‘ì†ìœ¼ë¡œ ê³„ì •ì´ ì ê²¼ìŠµë‹ˆë‹¤",
                        "PK P ê³„ì •ì´ ì ê²¨ìˆìŠµë‹ˆë‹¤"
                    ],
                    "PK P ì›¹ ì ‘ì† ì•ˆë¨": [
                        "PK P ì›¹ì‚¬ì´íŠ¸ì— ì ‘ì†ì´ ì•ˆë©ë‹ˆë‹¤",
                        "í†°ìº£ ì„œë¹„ìŠ¤ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤",
                        "PK P ì›¹ ì„œë¹„ìŠ¤ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
                    ],
                    "ê¸°íƒ€": [
                        "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        "ê¸°íƒ€ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
                    ]
                }
        except Exception as e:
            print(f"âŒ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _classify_by_keywords(self, customer_input: str) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (í´ë°±)"""
        try:
            normalized_input = customer_input.lower().strip()
            issue_scores = {}
            
            for issue_type, keywords in self.keyword_mapping.items():
                score = 0
                matched_keywords = []
                
                for keyword in keywords:
                    if keyword.lower() in normalized_input:
                        score += 1
                        matched_keywords.append(keyword)
                
                if score > 0:
                    issue_scores[issue_type] = {
                        'score': score,
                        'matched_keywords': matched_keywords,
                        'confidence': min(score / len(keywords), 1.0)
                    }
            
            if issue_scores:
                best_issue = max(issue_scores.items(), key=lambda x: x[1]['score'])
                best_issue_type = best_issue[0]
                best_score = best_issue[1]['score']
                best_confidence = best_issue[1]['confidence']
                matched_keywords = best_issue[1]['matched_keywords']
                
                confidence_level = 'high' if best_confidence >= 0.5 else 'medium' if best_confidence >= 0.3 else 'low'
                
                return {
                    'issue_type': best_issue_type,
                    'method': 'keyword_based',
                    'confidence': confidence_level,
                    'score': best_score,
                    'matched_keywords': matched_keywords,
                    'all_scores': {k: v['score'] for k, v in issue_scores.items()}
                }
            else:
                return {
                    'issue_type': 'ê¸°íƒ€',
                    'method': 'keyword_based',
                    'confidence': 'low',
                    'score': 0,
                    'matched_keywords': [],
                    'all_scores': {}
                }
                
        except Exception as e:
            return {
                'issue_type': 'ê¸°íƒ€',
                'method': 'keyword_based',
                'confidence': 'low',
                'error': str(e)
            }
    
    def classify_issue(self, customer_input: str, top_k: int = 3) -> Dict[str, Any]:
        """ë²¡í„° ê¸°ë°˜ ë¬¸ì œ ìœ í˜• ë¶„ë¥˜ (FAISS + í‚¤ì›Œë“œ í´ë°±)"""
        try:
            print(f"ğŸ” ë¶„ë¥˜ ì‹œë„: {customer_input}")
            
            # FAISS ë²¡í„° ë¶„ë¥˜ ì‹œë„
            if self.index is not None and self.embedding_model is not None:
                print("ğŸ”„ FAISS ë²¡í„° ë¶„ë¥˜ ì¤‘...")
                
                # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                query_embedding = self.embedding_model.encode([customer_input]).astype('float32')
                
                # FAISS ê²€ìƒ‰
                scores, indices = self.index.search(query_embedding, top_k)
                
                if len(indices[0]) > 0:
                    # ê²°ê³¼ ë¶„ì„
                    issue_scores = {}
                    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                        if idx < len(self.metadatas):
                            issue_type = self.metadatas[idx]['issue_type']
                            if issue_type not in issue_scores:
                                issue_scores[issue_type] = []
                            issue_scores[issue_type].append(float(score))
                    
                    # ìµœê³  ì ìˆ˜ ë¬¸ì œ ìœ í˜• ì„ íƒ
                    best_issue_type = 'ê¸°íƒ€'
                    best_score = 0
                    
                    for issue_type, scores_list in issue_scores.items():
                        max_score = max(scores_list)
                        if max_score > best_score:
                            best_score = max_score
                            best_issue_type = issue_type
                    
                    # ì‹ ë¢°ë„ ê²°ì •
                    if best_score >= 0.7:
                        confidence = 'high'
                    elif best_score >= 0.5:
                        confidence = 'medium'
                    else:
                        confidence = 'low'
                    
                    print(f"âœ… FAISS ë¶„ë¥˜ ê²°ê³¼: {best_issue_type} (ì ìˆ˜: {best_score:.3f}, ì‹ ë¢°ë„: {confidence})")
                    
                    return {
                        'issue_type': best_issue_type,
                        'method': 'faiss_vector',
                        'confidence': confidence,
                        'similarity_score': best_score,
                        'all_scores': {k: max(v) for k, v in issue_scores.items()}
                    }
            
            # FAISS ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë¡œ í´ë°±
            print("âš ï¸ FAISS ì‚¬ìš© ë¶ˆê°€, í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë¡œ í´ë°±")
            return self._classify_by_keywords(customer_input)
            
        except Exception as e:
            print(f"âŒ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            print("âš ï¸ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë¡œ í´ë°±")
            return self._classify_by_keywords(customer_input)
    
    def get_statistics(self) -> Dict[str, Any]:
        """ë¶„ë¥˜ê¸° í†µê³„"""
        if self.index is not None:
            return {
                'total_documents': len(self.documents),
                'issue_types': self.issue_types,
                'method': 'faiss_vector',
                'index_size': self.index.ntotal if self.index else 0
            }
        else:
            return {
                'total_documents': 0,
                'issue_types': self.issue_types,
                'method': 'keyword_only',
                'index_size': 0
            }

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("=== FAISS ë²¡í„° ë¶„ë¥˜ê¸° í…ŒìŠ¤íŠ¸ ===")
    
    classifier = FaissVectorClassifier()
    
    test_cases = [
        "ë¹„ë°€ë²ˆí˜¸ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤",
        "VMS í†µì‹  ì‹¤íŒ¨",
        "Ping í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨",
        "ì›¹ ì ‘ì† ì•ˆë¨"
    ]
    
    for test_input in test_cases:
        print(f"\n--- í…ŒìŠ¤íŠ¸: {test_input} ---")
        result = classifier.classify_issue(test_input)
        print(f"ê²°ê³¼: {result['issue_type']} ({result['method']}, {result['confidence']})")
    
    # í†µê³„
    stats = classifier.get_statistics()
    print(f"\n=== í†µê³„ ===")
    print(f"ì´ ë¬¸ì„œ ìˆ˜: {stats['total_documents']}")
    print(f"ë¶„ë¥˜ ë°©ë²•: {stats['method']}")
    print(f"ì¸ë±ìŠ¤ í¬ê¸°: {stats['index_size']}")
