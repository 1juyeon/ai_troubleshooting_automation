import os
import json
import pandas as pd
from typing import List, Dict, Any, Optional
import numpy as np
import re

# FAISS ì•ˆì „í•˜ê²Œ ì„í¬íŠ¸
try:
    import faiss
    FAISS_AVAILABLE = True
    print("âœ… FAISS ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    print(f"âš ï¸ FAISS ì„¤ì¹˜ í•„ìš”: pip install faiss-cpu")
    FAISS_AVAILABLE = False
    faiss = None
except Exception as e:
    print(f"âš ï¸ FAISS ì„í¬íŠ¸ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
    FAISS_AVAILABLE = False
    faiss = None

# sentence-transformers ì„í¬íŠ¸
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
    print("âœ… sentence-transformers ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    print(f"âš ï¸ sentence-transformers ì„¤ì¹˜ í•„ìš”: pip install sentence-transformers")
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    SentenceTransformer = None
except Exception as e:
    print(f"âš ï¸ sentence-transformers ì„í¬íŠ¸ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    SentenceTransformer = None

class ChromaVectorClassifier:
    def __init__(self, persist_directory: str = "faiss_issue_classification"):
        """FAISS ê¸°ë°˜ ë²¡í„° ë¶„ë¥˜ê¸° ì´ˆê¸°í™” (í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± í¬í•¨)"""
        self.persist_directory = persist_directory
        self.issue_types = [
            "í˜„ì¬ ë¹„ë°€ë²ˆí˜¸ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤",
            "VMSì™€ì˜ í†µì‹ ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤", 
            "Ping í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
            "Onvif ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤",
            "ë¡œê·¸ì¸ ì°¨ë‹¨ ìƒíƒœì…ë‹ˆë‹¤",
            "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
            "PK P ê³„ì • ë¡œê·¸ì¸ ì•ˆë¨",
            "PK P ì›¹ ì ‘ì† ì•ˆë¨",
            "ê¸°íƒ€"
        ]
        
        # í‚¤ì›Œë“œ ë§¤í•‘ ì •ì˜ (ChromaDB ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        self.keyword_mapping = {
            "í˜„ì¬ ë¹„ë°€ë²ˆí˜¸ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤": [
                "ë¹„ë°€ë²ˆí˜¸", "íŒ¨ìŠ¤ì›Œë“œ", "password", "ì¸ì¦", "ë¡œê·¸ì¸", "ì ‘ì†", "ì›¹", "cctv",
                "ë§ì§€ ì•Š", "í‹€ë ¸", "ì‹¤íŒ¨", "ì˜¤ë¥˜", "ì¸ì¦ ì‹¤íŒ¨", "ì ‘ì† ì‹¤íŒ¨", "ë¡œê·¸ì¸ ì‹¤íŒ¨"
            ],
            "VMSì™€ì˜ í†µì‹ ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤": [
                "vms", "VMS", "í†µì‹ ", "ì—°ê²°", "ì„œë²„", "svms", "nvr", "ì˜ìƒ", "ì¹´ë©”ë¼",
                "ì‹¤íŒ¨", "ì˜¤ë¥˜", "ì—°ê²° ì•ˆ", "í†µì‹  ì‹¤íŒ¨", "ì„œë²„ ì—°ê²°"
            ],
            "Ping í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤": [
                "ping", "Ping", "ë„¤íŠ¸ì›Œí¬", "ì—°ê²°", "í†µì‹ ", "í…ŒìŠ¤íŠ¸", "ì‘ë‹µ", "ë„¤íŠ¸ì›Œí¬ ì—°ê²°",
                "ì‹¤íŒ¨", "ì•ˆë¨", "ì˜¤ë¥˜", "ping ì‹¤íŒ¨", "ë„¤íŠ¸ì›Œí¬ ì‹¤íŒ¨"
            ],
            "Onvif ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤": [
                "onvif", "Onvif", "ONVIF", "í”„ë¡œí† ì½œ", "ì¹´ë©”ë¼", "ì„¤ì •", "í†µì‹ ", "ì‘ë‹µ",
                "ì—†", "ì‹¤íŒ¨", "ì˜¤ë¥˜", "onvif ì‘ë‹µ", "í”„ë¡œí† ì½œ ì‘ë‹µ"
            ],
            "ë¡œê·¸ì¸ ì°¨ë‹¨ ìƒíƒœì…ë‹ˆë‹¤": [
                "ì°¨ë‹¨", "ì ê¸ˆ", "ë¡œê·¸ì¸", "ê³„ì •", "cctv", "ì ‘ì†", "ì°¨ë‹¨ ìƒíƒœ", "ì ê¸ˆ ìƒíƒœ",
                "ì•ˆë¨", "ì‹¤íŒ¨", "ì˜¤ë¥˜", "ë¡œê·¸ì¸ ì°¨ë‹¨", "ê³„ì • ì°¨ë‹¨"
            ],
            "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤": [
                "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½", "íŒ¨ìŠ¤ì›Œë“œ ë³€ê²½", "ë³€ê²½", "ìˆ˜ì •", "ì—…ë°ì´íŠ¸", "ë¹„ë°€ë²ˆí˜¸ ìˆ˜ì •",
                "ì‹¤íŒ¨", "ì•ˆë¨", "ì˜¤ë¥˜", "ë³€ê²½ ì‹¤íŒ¨", "ìˆ˜ì • ì‹¤íŒ¨"
            ],
            "PK P ê³„ì • ë¡œê·¸ì¸ ì•ˆë¨": [
                "pk p", "PK P", "ê³„ì •", "ë¡œê·¸ì¸", "30ì¼", "ë¯¸ì ‘ì†", "ì ê¸ˆ", "ì ê²¼",
                "ì•ˆë¨", "ì‹¤íŒ¨", "ì˜¤ë¥˜", "ê³„ì • ë¡œê·¸ì¸", "pk p ê³„ì •"
            ],
            "PK P ì›¹ ì ‘ì† ì•ˆë¨": [
                "pk p", "PK P", "ì›¹", "ì ‘ì†", "ì›¹ì‚¬ì´íŠ¸", "í†°ìº£", "tomcat", "ì„œë¹„ìŠ¤",
                "ì•ˆë¨", "ì‹¤íŒ¨", "ì˜¤ë¥˜", "ì›¹ ì ‘ì†", "í˜ì´ì§€", "ë¡œë“œ"
            ]
        }
        
        # FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
        self.index = None
        self.documents = []
        self.metadatas = []
        self.embedding_model = None
        
        # ì˜ì¡´ì„± í™•ì¸
        if not FAISS_AVAILABLE:
            print("âŒ FAISSê°€ ì—†ì–´ ë²¡í„° ë¶„ë¥˜ê¸°ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("âš ï¸ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
            return
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_embedding_model()
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±
        self._load_or_create_index()
    
    def _initialize_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if SENTENCE_TRANSFORMERS_AVAILABLE:
                # ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš© (Windowsì—ì„œ ë” ì•ˆì •ì )
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                print("âœ… sentence-transformers ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                
                # ì„ë² ë”© í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
                test_embedding = self.embedding_model.encode(["í…ŒìŠ¤íŠ¸ ë¬¸ì¥"])
                print(f"âœ… ì„ë² ë”© í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(test_embedding[0])}ì°¨ì›")
            else:
                print("âš ï¸ sentence-transformers ì—†ìŒ, í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë§Œ ì‚¬ìš©")
                self.embedding_model = None
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.embedding_model = None
    
    def _load_or_create_index(self):
        """FAISS ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        try:
            if not FAISS_AVAILABLE or not self.embedding_model:
                print("âš ï¸ FAISS ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ ì—†ìŒ, í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë§Œ ì‚¬ìš©")
                return
            
            # ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            index_path = os.path.join(self.persist_directory, "faiss_index.bin")
            metadata_path = os.path.join(self.persist_directory, "metadata.json")
            
            if os.path.exists(index_path) and os.path.exists(metadata_path):
                print("ğŸ”„ ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
                self.index = faiss.read_index(index_path)
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.documents = data['documents']
                    self.metadatas = data['metadatas']
                print(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
            else:
                print("ğŸ”„ ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
                self._create_index()
                
        except Exception as e:
            print(f"âŒ FAISS ì¸ë±ìŠ¤ ë¡œë“œ/ìƒì„± ì‹¤íŒ¨: {e}")
            self.index = None
    
    def _create_index(self):
        """ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        try:
            # ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
            sample_data = self._load_sample_data()
            
            if not sample_data:
                print("âŒ ìƒ˜í”Œ ë°ì´í„° ì—†ìŒ")
                return
            
            # ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            documents = []
            metadatas = []
            
            for issue_type, samples in sample_data.items():
                for i, sample in enumerate(samples):
                    documents.append(sample)
                    metadatas.append({
                        "issue_type": issue_type,
                        "sample_index": i,
                        "is_sample": True
                    })
            
            # ì„ë² ë”© ìƒì„±
            print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = self.embedding_model.encode(documents)
            embeddings = embeddings.astype('float32')
            
            # FAISS ì¸ë±ìŠ¤ ìƒì„± (ë‚´ì  ê¸°ë°˜ ìœ ì‚¬ë„)
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            self.index.add(embeddings)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            self.documents = documents
            self.metadatas = metadatas
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.persist_directory, exist_ok=True)
            
            # ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(self.index, os.path.join(self.persist_directory, "faiss_index.bin"))
            with open(os.path.join(self.persist_directory, "metadata.json"), 'w', encoding='utf-8') as f:
                json.dump({
                    'documents': self.documents,
                    'metadatas': self.metadatas
                }, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
            
        except Exception as e:
            print(f"âŒ FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            self.index = None
    
    def _load_sample_data(self):
        """ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ (Streamlit Cloud í˜¸í™˜)"""
        try:
            # Streamlit resourceë¥¼ í†µí•œ íŒŒì¼ ì½ê¸° ì‹œë„
            try:
                import streamlit as st
                # Streamlit Cloudì—ì„œ íŒŒì¼ ì½ê¸°
                with open("vector_data/sample_issues.json", 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    print("âœ… Streamlit resourceë¡œ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ì„±ê³µ")
                    return data.get("sample_issues", {})
            except:
                pass
            
            # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„ (ë¡œì»¬ í™˜ê²½ í˜¸í™˜)
            possible_paths = [
                os.path.join(os.path.dirname(__file__), "vector_data", "sample_issues.json"),
                os.path.join("vector_data", "sample_issues.json"),
                "vector_data/sample_issues.json",
                os.path.join(os.getcwd(), "vector_data", "sample_issues.json")
            ]
            
            for json_path in possible_paths:
                if os.path.exists(json_path):
                    print(f"âœ… ìƒ˜í”Œ ë°ì´í„° íŒŒì¼ ë°œê²¬: {json_path}")
                    with open(json_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        return data.get("sample_issues", {})
            
            print("âš ï¸ ìƒ˜í”Œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ ë°ì´í„° ì‚¬ìš©")
            print("ğŸ“Š ê¸°ë³¸ ë°ì´í„°ë¡œ 54ê°œ ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±")
            # ê¸°ë³¸ ë°ì´í„°
            return {
                "í˜„ì¬ ë¹„ë°€ë²ˆí˜¸ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤": [
                    "CCTV ì›¹ ì ‘ì† ì‹œ ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ ì‹¤íŒ¨",
                    "ì €ì¥ëœ ë¹„ë°€ë²ˆí˜¸ë¡œ ë¡œê·¸ì¸ì´ ì•ˆë©ë‹ˆë‹¤",
                        "ë¹„ë°€ë²ˆí˜¸ë¥¼ ì •í™•íˆ ì…ë ¥í–ˆëŠ”ë°ë„ ì¸ì¦ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤",
                        "CCTV ì›¹ ë¡œê·¸ì¸ ì‹œ ì ‘ì† ì‹¤íŒ¨",
                        "íŒ¨ìŠ¤ì›Œë“œê°€ ë§ì§€ ì•Šì•„ ë¡œê·¸ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        "ì›¹ ì ‘ì† ì‹œ ì¸ì¦ ì‹¤íŒ¨ê°€ ê³„ì† ë°œìƒí•©ë‹ˆë‹¤"
                    ],
                    "VMSì™€ì˜ í†µì‹ ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤": [
                        "VMS ì„œë²„ì™€ì˜ ì—°ê²°ì´ ì•ˆë©ë‹ˆë‹¤",
                        "VMS íŒ¨ìŠ¤ì›Œë“œê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤",
                        "SVMS í†µì‹  ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤",
                        "NVRê³¼ VMS ê°„ í†µì‹  ì‹¤íŒ¨",
                        "VMS ì„¤ì •ì—ì„œ ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤",
                        "VMS ì„œë²„ ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤"
                    ],
                    "Ping í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤": [
                        "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ì•ˆë©ë‹ˆë‹¤",
                        "Ping í…ŒìŠ¤íŠ¸ì—ì„œ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤",
                        "ë„¤íŠ¸ì›Œí¬ í†µì‹ ì´ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤",
                        "ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        "ë„¤íŠ¸ì›Œí¬ ì ê²€ì—ì„œ ì‹¤íŒ¨ê°€ ë°œìƒí•©ë‹ˆë‹¤",
                        "í†µì‹  í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
                    ],
                    "Onvif ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤": [
                        "Onvif í”„ë¡œí† ì½œ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤",
                        "ì¹´ë©”ë¼ì™€ Onvif í†µì‹ ì´ ì•ˆë©ë‹ˆë‹¤",
                        "Onvif ì„¤ì •ì—ì„œ ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤",
                        "HTTP/HTTPS í”„ë¡œí† ì½œ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤",
                        "ì¹´ë©”ë¼ í†µì‹  í”„ë¡œí† ì½œ ì˜¤ë¥˜",
                        "Onvif ì„œë¹„ìŠ¤ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
                    ],
                    "ë¡œê·¸ì¸ ì°¨ë‹¨ ìƒíƒœì…ë‹ˆë‹¤": [
                        "CCTV ë¡œê·¸ì¸ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤",
                        "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ í›„ ë¡œê·¸ì¸ ì°¨ë‹¨ ìƒíƒœì…ë‹ˆë‹¤",
                        "ê³„ì •ì´ ì°¨ë‹¨ë˜ì–´ ë¡œê·¸ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        "CCTV ì°¨ë‹¨ ìƒíƒœë¡œ ì ‘ì†ì´ ì•ˆë©ë‹ˆë‹¤",
                        "ë¡œê·¸ì¸ ì‹œë„ê°€ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤",
                        "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ìœ¼ë¡œ ì¸í•œ ì°¨ë‹¨ ìƒíƒœì…ë‹ˆë‹¤"
                    ],
                    "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤": [
                        "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ì´ ì•ˆë©ë‹ˆë‹¤",
                        "íŒ¨ìŠ¤ì›Œë“œ ìˆ˜ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
                        "ë¹„ë°€ë²ˆí˜¸ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤",
                        "ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ ì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤",
                        "íŒ¨ìŠ¤ì›Œë“œ ë³€ê²½ í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
                        "ë¹„ë°€ë²ˆí˜¸ ìˆ˜ì •ì´ ì œëŒ€ë¡œ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
                    ],
                    "PK P ê³„ì • ë¡œê·¸ì¸ ì•ˆë¨": [
                        "PK P ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸ì´ ì•ˆë©ë‹ˆë‹¤",
                        "30ì¼ ë¯¸ì ‘ì†ìœ¼ë¡œ ê³„ì •ì´ ì ê²¼ìŠµë‹ˆë‹¤",
                        "PK P ê³„ì •ì´ ì ê²¨ìˆìŠµë‹ˆë‹¤",
                        "ê³„ì • ë¡œê·¸ì¸ì— ì‹¤íŒ¨í•©ë‹ˆë‹¤",
                        "PK P ê³„ì • ì¸ì¦ì´ ì•ˆë©ë‹ˆë‹¤",
                        "ê³„ì • ì ‘ì†ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤"
                    ],
                    "PK P ì›¹ ì ‘ì† ì•ˆë¨": [
                        "PK P ì›¹ì‚¬ì´íŠ¸ì— ì ‘ì†ì´ ì•ˆë©ë‹ˆë‹¤",
                        "í†°ìº£ ì„œë¹„ìŠ¤ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤",
                        "PK P ì›¹ ì„œë¹„ìŠ¤ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
                        "ì›¹ ì ‘ì† ì‹œ ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤",
                        "PK P ì›¹ í˜ì´ì§€ê°€ ë¡œë“œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
                        "ì›¹ ì„œë¹„ìŠ¤ ì ‘ì†ì— ì‹¤íŒ¨í•©ë‹ˆë‹¤"
                    ],
                    "ê¸°íƒ€": [
                        "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        "ê¸°íƒ€ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        "ë¬¸ì œë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        "ê¸°íƒ€ ê¸°ìˆ ì  ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        "ë¶„ë¥˜ë˜ì§€ ì•ŠëŠ” ë¬¸ì œì…ë‹ˆë‹¤"
                    ]
                }
        except Exception as e:
            print(f"âŒ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _classify_by_keywords(self, customer_input: str) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì œ ìœ í˜• ë¶„ë¥˜ (ChromaDB ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)"""
        try:
            print(f"ğŸ” í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ ì‹œë„: {customer_input}")
            
            # ì…ë ¥ í…ìŠ¤íŠ¸ ì •ê·œí™”
            normalized_input = customer_input.lower().strip()
            
            # ê° ë¬¸ì œ ìœ í˜•ë³„ë¡œ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            issue_scores = {}
            
            for issue_type, keywords in self.keyword_mapping.items():
                score = 0
                matched_keywords = []
                
                for keyword in keywords:
                    if keyword.lower() in normalized_input:
                        score += 1
                        matched_keywords.append(keyword)
                
                if score > 0:
                    issue_scores[issue_type] = {
                        'score': score,
                        'matched_keywords': matched_keywords,
                        'confidence': min(score / len(keywords), 1.0)
                    }
            
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì€ ë¬¸ì œ ìœ í˜• ì„ íƒ
            if issue_scores:
                best_issue = max(issue_scores.items(), key=lambda x: x[1]['score'])
                best_issue_type = best_issue[0]
                best_score = best_issue[1]['score']
                best_confidence = best_issue[1]['confidence']
                matched_keywords = best_issue[1]['matched_keywords']
                
                # ì‹ ë¢°ë„ ê²°ì •
                if best_confidence >= 0.3:  # 30% ì´ìƒ í‚¤ì›Œë“œ ë§¤ì¹­
                    confidence_level = 'high' if best_confidence >= 0.5 else 'medium'
                else:
                    confidence_level = 'low'
                
                print(f"âœ… í‚¤ì›Œë“œ ë¶„ë¥˜ ê²°ê³¼: {best_issue_type} (ì ìˆ˜: {best_score}, ì‹ ë¢°ë„: {confidence_level})")
                print(f"ğŸ”‘ ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {matched_keywords}")
                
                return {
                    'issue_type': best_issue_type,
                    'method': 'keyword_based',
                    'confidence': confidence_level,
                    'score': best_score,
                    'matched_keywords': matched_keywords,
                    'all_scores': {k: v['score'] for k, v in issue_scores.items()}
                }
            else:
                print("âŒ ë§¤ì¹­ë˜ëŠ” í‚¤ì›Œë“œê°€ ì—†ìŒ, ê¸°íƒ€ë¡œ ë¶„ë¥˜")
                return {
                    'issue_type': 'ê¸°íƒ€',
                    'method': 'keyword_based',
                    'confidence': 'low',
                    'score': 0,
                    'matched_keywords': [],
                    'all_scores': {}
                }
                
        except Exception as e:
            print(f"âŒ í‚¤ì›Œë“œ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {
                'issue_type': 'ê¸°íƒ€',
                'method': 'keyword_based',
                'confidence': 'low',
                'error': str(e)
            }
    
    def classify_issue(self, customer_input: str, top_k: int = 3) -> Dict[str, Any]:
        """FAISS ë²¡í„° ê¸°ë°˜ ë¬¸ì œ ìœ í˜• ë¶„ë¥˜ (í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± í¬í•¨)"""
        try:
            print(f"ğŸ” ë¶„ë¥˜ ì‹œë„: {customer_input}")
            print(f"ğŸ“Š FAISS ì¸ë±ìŠ¤ ìƒíƒœ: {self.index is not None}")
            print(f"ğŸ§  ì„ë² ë”© ëª¨ë¸ ìƒíƒœ: {self.embedding_model is not None}")
            
            # FAISS ë²¡í„° ë¶„ë¥˜ ì‹œë„
            if self.index is not None and self.embedding_model is not None:
                print("ğŸ”„ FAISS ë²¡í„° ë¶„ë¥˜ ì¤‘...")
                
                # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                query_embedding = self.embedding_model.encode([customer_input]).astype('float32')
                print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(query_embedding[0])}ì°¨ì›")
                
                # FAISS ê²€ìƒ‰
                print("ğŸ” FAISS ê²€ìƒ‰ ì¤‘...")
                scores, indices = self.index.search(query_embedding, top_k)
                print(f"âœ… FAISS ê²€ìƒ‰ ì™„ë£Œ: {len(indices[0])}ê°œ ê²°ê³¼")
                
                if len(indices[0]) > 0:
                    # ê²°ê³¼ ë¶„ì„
                    issue_scores = {}
                    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                        if idx < len(self.metadatas):
                            issue_type = self.metadatas[idx]['issue_type']
                            if issue_type not in issue_scores:
                                issue_scores[issue_type] = []
                            issue_scores[issue_type].append(float(score))
                    
                    # ìµœê³  ì ìˆ˜ ë¬¸ì œ ìœ í˜• ì„ íƒ
                    best_issue_type = 'ê¸°íƒ€'
                    best_score = 0
                    
                    for issue_type, scores_list in issue_scores.items():
                        max_score = max(scores_list)
                        if max_score > best_score:
                            best_score = max_score
                            best_issue_type = issue_type
                    
                    # ì‹ ë¢°ë„ ê²°ì •
                    if best_score >= 0.7:
                        confidence = 'high'
                    elif best_score >= 0.5:
                        confidence = 'medium'
                    else:
                        confidence = 'low'
                    
                    print(f"âœ… FAISS ë¶„ë¥˜ ê²°ê³¼: {best_issue_type} (ì ìˆ˜: {best_score:.3f}, ì‹ ë¢°ë„: {confidence})")
                    
                    return {
                        'issue_type': best_issue_type,
                        'method': 'faiss_vector',
                        'confidence': confidence,
                        'similarity_score': best_score,
                        'all_scores': {k: max(v) for k, v in issue_scores.items()},
                        'top_matches': [
                            {
                                'document': self.documents[indices[0][i]],
                                'issue_type': self.metadatas[indices[0][i]]['issue_type'],
                                'similarity': float(scores[0][i])
                            }
                            for i in range(len(indices[0]))
                        ]
                    }
            
            # FAISS ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë¡œ í´ë°±
            print("âš ï¸ FAISS ì‚¬ìš© ë¶ˆê°€, í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë¡œ í´ë°±")
            return self._classify_by_keywords(customer_input)
            
        except Exception as e:
            print(f"âŒ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            print("âš ï¸ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë¡œ í´ë°±")
            return self._classify_by_keywords(customer_input)
    
    def add_training_data(self, customer_input: str, issue_type: str, metadata: Dict[str, Any] = None):
        """ìƒˆë¡œìš´ í•™ìŠµ ë°ì´í„° ì¶”ê°€ (FAISS)"""
        try:
            if not self.index or not self.embedding_model:
                print("âš ï¸ FAISS ì¸ë±ìŠ¤ ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ ì—†ìŒ, í•™ìŠµ ë°ì´í„° ì¶”ê°€ ë¶ˆê°€")
                return False
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            if metadata is None:
                metadata = {}
            metadata.update({
                'issue_type': issue_type,
                'is_sample': False,
                'added_timestamp': str(pd.Timestamp.now())
            })
            
            # ì„ë² ë”© ìƒì„±
            embedding = self.embedding_model.encode([customer_input]).astype('float32')
            
            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.index.add(embedding)
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self.documents.append(customer_input)
            self.metadatas.append(metadata)
            
            # ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(self.index, os.path.join(self.persist_directory, "faiss_index.bin"))
            with open(os.path.join(self.persist_directory, "metadata.json"), 'w', encoding='utf-8') as f:
                json.dump({
                    'documents': self.documents,
                    'metadatas': self.metadatas
                }, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… í•™ìŠµ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ: {issue_type}")
            return True
            
        except Exception as e:
            print(f"âŒ í•™ìŠµ ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """FAISS ë²¡í„° DB í†µê³„"""
        try:
            if self.index is not None:
                # ë¬¸ì œ ìœ í˜•ë³„ í†µê³„
                issue_type_counts = {}
                for metadata in self.metadatas:
                    issue_type = metadata.get('issue_type', 'ê¸°íƒ€')
                    issue_type_counts[issue_type] = issue_type_counts.get(issue_type, 0) + 1
                
                return {
                    'total_documents': len(self.documents),
                    'issue_types': list(issue_type_counts.keys()),
                    'issue_type_counts': issue_type_counts,
                    'index_size': self.index.ntotal,
                    'method': 'faiss_vector',
                    'embedding_model': 'all-MiniLM-L6-v2' if self.embedding_model else 'None',
                    'collection_name': 'FAISS Vector DB'
                }
            else:
                # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ê¸° í†µê³„
                sample_data = self._load_sample_data()
                total_samples = sum(len(samples) for samples in sample_data.values())
                issue_type_counts = {issue_type: len(samples) for issue_type, samples in sample_data.items()}
                
                return {
                    'total_documents': total_samples,
                    'issue_types': list(issue_type_counts.keys()),
                    'issue_type_counts': issue_type_counts,
                    'index_size': 0,
                    'method': 'keyword_only',
                    'embedding_model': 'None',
                    'collection_name': 'N/A'
                }
            
        except Exception as e:
            print(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'total_documents': 0,
                'issue_types': self.issue_types,
                'issue_type_counts': {},
                'index_size': 0,
                'method': 'error',
                'error': str(e)
            }
    
    def clear_database(self):
        """FAISS ë²¡í„° DB ì´ˆê¸°í™”"""
        try:
            # ì €ì¥ëœ íŒŒì¼ë“¤ ì‚­ì œ
            index_path = os.path.join(self.persist_directory, "faiss_index.bin")
            metadata_path = os.path.join(self.persist_directory, "metadata.json")
            
            if os.path.exists(index_path):
                os.remove(index_path)
            if os.path.exists(metadata_path):
                os.remove(metadata_path)
            
            # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            self.index = None
            self.documents = []
            self.metadatas = []
            
            # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
            self._load_or_create_index()
            print("âœ… FAISS ë²¡í„° DB ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ FAISS ë²¡í„° DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # ë²¡í„° ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
    classifier = ChromaVectorClassifier()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_cases = [
        "PK Pì— ì €ì¥ëœ ë¹„ë°€ë²ˆí˜¸ë¡œ CCTV ì›¹ ì ‘ì†ì´ ì•ˆë©ë‹ˆë‹¤. ë¹„ë°€ë²ˆí˜¸ëŠ” ì •í™•íˆ ì…ë ¥í–ˆëŠ”ë°ë„ ì¸ì¦ ì‹¤íŒ¨ê°€ ë°œìƒí•©ë‹ˆë‹¤.",
        "VMSì™€ì˜ í†µì‹ ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. VMS íŒ¨ìŠ¤ì›Œë“œê°€ ë§ì§€ ì•ŠëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
        "Ping í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ì•ˆë˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
        "Onvif í”„ë¡œí† ì½œë¡œ ì¹´ë©”ë¼ì™€ í†µì‹ ì´ ì•ˆë©ë‹ˆë‹¤.",
        "PK P ê³„ì •ì´ 30ì¼ ë¯¸ì ‘ì†ìœ¼ë¡œ ì ê²¼ìŠµë‹ˆë‹¤."
    ]
    
    print("\n=== ë²¡í„° ê¸°ë°˜ ë¬¸ì œ ìœ í˜• ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ ===")
    for i, test_input in enumerate(test_cases, 1):
        print(f"\n--- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i} ---")
        print(f"ì…ë ¥: {test_input}")
        
        result = classifier.classify_issue(test_input)
        print(f"ë¶„ë¥˜ ê²°ê³¼: {result['issue_type']}")
        print(f"ì‹ ë¢°ë„: {result['confidence']}")
        similarity_score = result.get('similarity_score', 'N/A')
        if isinstance(similarity_score, (int, float)):
            print(f"ìœ ì‚¬ë„ ì ìˆ˜: {similarity_score:.3f}")
        else:
            print(f"ìœ ì‚¬ë„ ì ìˆ˜: {similarity_score}")
        
        if 'top_matches' in result:
            print("ìƒìœ„ ë§¤ì¹­ ê²°ê³¼:")
            for match in result['top_matches'][:2]:
                similarity = match.get('similarity', 'N/A')
                if isinstance(similarity, (int, float)):
                    print(f"  - {match['issue_type']}: {similarity:.3f}")
                else:
                    print(f"  - {match['issue_type']}: {similarity}")
    
    # í†µê³„ ì¶œë ¥
    print("\n=== ë²¡í„° DB í†µê³„ ===")
    stats = classifier.get_statistics()
    print(f"ì´ ë¬¸ì„œ ìˆ˜: {stats['total_documents']}")
    print(f"ë¬¸ì œ ìœ í˜•: {stats['issue_types']}")
    if 'issue_type_counts' in stats:
        print("ë¬¸ì œ ìœ í˜•ë³„ ë¬¸ì„œ ìˆ˜:")
        for issue_type, count in stats['issue_type_counts'].items():
            print(f"  - {issue_type}: {count}ê°œ")
